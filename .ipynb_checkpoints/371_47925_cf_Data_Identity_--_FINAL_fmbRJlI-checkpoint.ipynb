{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pd.set_option('max_columns',100)\n",
    "import datetime\n",
    "\n",
    "import os\n",
    "os.chdir('/Users/shubhamjain/Downloads/AV/Student Identity/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data...\n"
     ]
    }
   ],
   "source": [
    "print (\"loading data...\")\n",
    "train = pd.read_csv('train_HK6lq50.csv')\n",
    "test = pd.read_csv('test_2nAIblo.csv')\n",
    "\n",
    "sample = pd.read_csv('sample_submission_vaSxamm.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## filling missing values of age \n",
    "\n",
    "train.loc[(train['education'] == 'Matriculation') & (train['age'].isnull()), 'age'] = 36\n",
    "train.loc[(train['education'] == 'Bachelors') & (train['age'].isnull()), 'age'] = 39\n",
    "train.loc[(train['education'] == 'High School Diploma') & (train['age'].isnull()), 'age'] = 35\n",
    "train.loc[(train['education'] == 'Masters') & (train['age'].isnull()), 'age'] = 43\n",
    "train.loc[(train['education'] == 'No Qualification') & (train['age'].isnull()), 'age'] = 34\n",
    "\n",
    "test.loc[(test['education'] == 'Matriculation') & (test['age'].isnull()), 'age'] = 36\n",
    "test.loc[(test['education'] == 'Bachelors') & (test['age'].isnull()), 'age'] = 39\n",
    "test.loc[(test['education'] == 'High School Diploma') & (test['age'].isnull()), 'age'] = 35\n",
    "test.loc[(test['education'] == 'Masters') & (test['age'].isnull()), 'age'] = 43\n",
    "test.loc[(test['education'] == 'No Qualification') & (test['age'].isnull()), 'age'] = 34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['trainee_engagement_rating'].fillna(2, inplace=True)\n",
    "test['trainee_engagement_rating'].fillna(2, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['program_number'] = train.program_id.apply(lambda x: str(x).split('_')[1])\n",
    "test['program_number'] = test.program_id.apply(lambda x: str(x).split('_')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## mean encoder of program_id\n",
    "\n",
    "maper = train.groupby('program_id')['is_pass'].mean().reset_index()\n",
    "maper = maper.rename(columns = {'is_pass': 'program_id_mean'})\n",
    "\n",
    "train = train.merge(maper, on = ['program_id'], how='left')\n",
    "test = test.merge(maper, on = ['program_id'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## avg passing age of trainee of each program id\n",
    "\n",
    "mapper = train[train['is_pass'] == 1].groupby('program_id')['age'].mean().round().astype('int')\n",
    "\n",
    "train['avg_passing_age'] = train['program_id']\n",
    "test['avg_passing_age'] = test['program_id']\n",
    "\n",
    "train['avg_passing_age'] = train['avg_passing_age'].map(mapper)\n",
    "test['avg_passing_age'] = test['avg_passing_age'].map(mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## program id and city tier\n",
    "\n",
    "mapper = train.groupby(['program_id', 'city_tier'])['is_pass'].mean().reset_index()\n",
    "mapper = mapper.rename(columns = {'is_pass': 'program_city_pass'})\n",
    "\n",
    "train =train.merge(mapper, on = ['program_id','city_tier'], how='left')\n",
    "test = test.merge(mapper, on = ['program_id','city_tier'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### avg, max attempts require for each program id\n",
    "\n",
    "temp = train[train['is_pass'] == 0].groupby('program_id')['trainee_id'].value_counts()\n",
    "temp = pd.DataFrame(temp).rename(columns = {'trainee_id': 'attempts'}).reset_index()\n",
    "\n",
    "mean = temp.groupby('program_id')['attempts'].mean().round(2).reset_index().rename(columns = {'attempts':'mean_attempts'})\n",
    "maxi = temp.groupby('program_id')['attempts'].max().round(2).reset_index().rename(columns = {'attempts':'max_attempts'})\n",
    "\n",
    "train = train.merge(mean, on = ['program_id'], how='left')\n",
    "test = test.merge(mean, on = ['program_id'], how='left')\n",
    "\n",
    "train = train.merge(maxi, on = ['program_id'], how='left')\n",
    "test = test.merge(maxi, on = ['program_id'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = train.sort_values(by = ['trainee_id','id'], ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## percentage of trainee who cleared it in first one\n",
    "\n",
    "temp = train.groupby(['program_id','trainee_id'])['is_pass'].apply(lambda x: x.head(1)).reset_index()\n",
    "temp = temp.groupby('program_id')['is_pass'].mean().reset_index().rename(columns = {'is_pass':'first_attempt'})\n",
    "\n",
    "train = train.merge(temp, on = ['program_id'], how='left')\n",
    "test = test.merge(temp, on = ['program_id'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## avg rating of each trainee\n",
    "\n",
    "combi = pd.concat([train,test],axis=0)\n",
    "temp = combi.groupby('trainee_id')['trainee_engagement_rating'].mean().reset_index().rename(columns = {'trainee_engagement_rating':\n",
    "                                                                                                'avg_rating'})\n",
    "\n",
    "train = train.merge(temp, on = ['trainee_id'], how='left')\n",
    "test = test.merge(temp, on = ['trainee_id'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training = train.copy()\n",
    "testing = test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###### one hot encoding\n",
    "feat = ['program_type', 'education', 'program_number']\n",
    "\n",
    "dummies = pd.get_dummies(training[feat], prefix = feat)\n",
    "training = pd.concat([training, dummies], axis=1)\n",
    "\n",
    "dummies = pd.get_dummies(testing[feat], prefix = feat)\n",
    "testing = pd.concat([testing, dummies], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## mean encoding test_id \n",
    "\n",
    "mapper = training.groupby('test_id')['is_pass'].mean()\n",
    "\n",
    "training['test_id'] = training['test_id'].map(mapper)\n",
    "testing['test_id'] = testing['test_id'].map(mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## label encoding\n",
    "\n",
    "feat = ['test_type', 'gender', 'is_handicapped']\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "lb = LabelEncoder()\n",
    "\n",
    "for i in feat:\n",
    "    training[i] = lb.fit_transform(training[i].astype('str'))\n",
    "    testing[i] = lb.transform(testing[i].astype('str'))\n",
    "    \n",
    "    training[i] = training[i].astype('object')\n",
    "    testing[i] = testing[i].astype('object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## for difficulty level\n",
    "#training.difficulty_level.value_counts()\n",
    "dic = {'easy':1, 'intermediate':2, 'hard':3, 'vary hard': 4}\n",
    "\n",
    "training['difficulty_level'] = training['difficulty_level'].map(dic)\n",
    "testing['difficulty_level'] = testing['difficulty_level'].map(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## for training features\n",
    "\n",
    "## trainee id sum encoder\n",
    "\n",
    "temp = training.groupby('trainee_id')['is_pass'].sum().reset_index().rename(columns = {'is_pass':'trainee_pass_sum'})\n",
    "\n",
    "training = training.merge(temp, on = ['trainee_id'], how='left')\n",
    "testing = testing.merge(temp, on = ['trainee_id'], how='left')\n",
    "\n",
    "testing.fillna(-1, inplace=True)\n",
    "\n",
    "\n",
    "## trainee last result\n",
    "\n",
    "temp = training.groupby(['trainee_id'])['is_pass'].apply(lambda x: x.tail(1)).reset_index().rename(columns = {'is_pass':'last_result'})\n",
    "temp.drop('level_1',axis=1,inplace=True)\n",
    "\n",
    "training = training.merge(temp, on = ['trainee_id'], how='left')\n",
    "testing = testing.merge(temp, on = ['trainee_id'], how='left')\n",
    "\n",
    "testing.fillna(-1, inplace=True)\n",
    "\n",
    "## trainee attempt number\n",
    "\n",
    "temp = training.groupby(['trainee_id','program_id'])['is_pass'].count().reset_index().rename(columns = {'is_pass':'trainee_attempts'})\n",
    "\n",
    "                                                                                            \n",
    "training = training.merge(temp, on = ['trainee_id','program_id'], how='left')\n",
    "testing = testing.merge(temp, on = ['trainee_id','program_id'], how='left')\n",
    "\n",
    "testing.fillna(-1, inplace=True)                                                                                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## program type and education\n",
    "temp = training.groupby(['program_type','education'])['is_pass'].mean().reset_index().rename(columns = {'is_pass':'type_education'})\n",
    "\n",
    "training = training.merge(temp, on = ['program_type','education'], how='left')\n",
    "testing = testing.merge(temp, on = ['program_type','education'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## percentile of course passed by him\n",
    "\n",
    "temp = training.groupby(['program_id','trainee_id'])['is_pass'].sum().groupby('program_id').max().reset_index()\n",
    "\n",
    "temp = temp.rename(columns = {'is_pass': 'total_tests'})\n",
    "\n",
    "training = training.merge(temp, on ='program_id', how='left')\n",
    "testing = testing.merge(temp, on ='program_id', how='left')\n",
    "\n",
    "training['%_program_cleared'] = training['trainee_pass_sum'] / training['total_tests']\n",
    "testing['%_program_cleared'] = testing['trainee_pass_sum'] / testing['total_tests']\n",
    "\n",
    "\n",
    "training['%_program_cleared'] = (training['%_program_cleared'] -training['%_program_cleared'].min()) /(training['%_program_cleared'].max() -training['%_program_cleared'].min())\n",
    "testing['%_program_cleared'] = (testing['%_program_cleared'] -testing['%_program_cleared'].min())/ (testing['%_program_cleared'].max() -testing['%_program_cleared'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training.to_csv('train_df.csv', index=False)\n",
    "testing.to_csv('test_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = list(training.drop(['id', 'program_id','program_type', 'trainee_id','education', 'is_pass'\n",
    "                              ],axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in training[features].columns:\n",
    "    if (training[i].dtype == 'object'):\n",
    "        training[i] = training[i].astype('int')\n",
    "        \n",
    "for i in testing[features].columns:\n",
    "    if (testing[i].dtype == 'object'):\n",
    "        testing[i] = testing[i].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import  roc_auc_score,roc_auc_score\n",
    "def runXGB(train_X, train_y, test_X, test_y=None, feature_names=None, seed_val=0,max_depth=10):\n",
    "        params = {}\n",
    "        params[\"objective\"] = \"binary:logistic\"\n",
    "        params['eval_metric'] = 'auc'\n",
    "        params[\"eta\"] = 0.01 #0.00334\n",
    "        params[\"min_child_weight\"] = 1\n",
    "        params[\"subsample\"] = 0.8\n",
    "        params[\"colsample_bytree\"] = 0.3\n",
    "        params[\"silent\"] = 1\n",
    "        params[\"max_depth\"] = max_depth\n",
    "        params[\"seed\"] = seed_val\n",
    "        #params[\"max_delta_step\"] = 2\n",
    "        params[\"verbose\"] = 100\n",
    "        num_rounds = 5000 #2500\n",
    "\n",
    "        plst = list(params.items())\n",
    "        xgtrain = xgb.DMatrix(train_X, label=train_y)\n",
    "\n",
    "        if test_y is not None:\n",
    "            xgtest = xgb.DMatrix(test_X, label=test_y)\n",
    "            watchlist = [ (xgtrain,'train'), (xgtest, 'test') ]\n",
    "            model = xgb.train(plst , xgtrain, num_rounds, watchlist, early_stopping_rounds= 500)\n",
    "        else:\n",
    "            xgtest = xgb.DMatrix(test_X)\n",
    "            #xgtest1 = xgb.DMatrix(test_X1)\n",
    "            model = xgb.train(plst, xgtrain, 2000)\n",
    "        #print feature_names,if feature_names\n",
    "        if feature_names:\n",
    "            create_feature_map(feature_names)\n",
    "            model.dump_model('xgbmodel.txt', 'xgb.fmap', with_stats=True)\n",
    "            importance = model.get_fscore(fmap='xgb.fmap')\n",
    "            importance = sorted(importance.items(), key=operator.itemgetter(1), reverse=True)\n",
    "            imp_df = pd.DataFrame(importance, columns=['feature','fscore'])\n",
    "            imp_df['fscore'] = imp_df['fscore'] / imp_df['fscore'].sum()\n",
    "            imp_df.to_csv(\"imp_feat.txt\", index=False)\n",
    "\n",
    "        pred_test_y = model.predict(xgtest)\n",
    "        \n",
    "        if test_y is not None:\n",
    "            loss = roc_auc_score(test_y, pred_test_y)\n",
    "            print (loss)\n",
    "            return (pred_test_y, loss)\n",
    "        else:\n",
    "            return (pred_test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred1 = runXGB(training[features], y, testing[features],max_depth=10)\n",
    "pred2 = runXGB(training[features], y, testing[features],max_depth=6)\n",
    "pred3 = runXGB(training[features], y, testing[features],max_depth=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = (pred1 + pred2 + pred3)/3\n",
    "sample['is_pass'] = pred\n",
    "sample.to_csv('sub/xgb.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGB Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train_df.csv')\n",
    "test_df = pd.read_csv('test_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = list(train_df.drop(['id', 'program_id','program_type', 'trainee_id','education', 'is_pass'\n",
    "                              ],axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = train_df['is_pass'].ravel()\n",
    "#train = training[features]\n",
    "x_train = train_df[features].values # Creates an array of the train data\n",
    "x_test = test_df[features].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create 5 objects that represent our 4 models\n",
    "SEED = 0\n",
    "NFOLDS = 5\n",
    "\n",
    "ntrain = train_df.shape[0]\n",
    "ntest = test_df.shape[0]\n",
    "\n",
    "from sklearn.cross_validation import KFold\n",
    "kf = KFold(ntrain, n_folds= NFOLDS, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "def runXGB(train_X, train_y, test_X, weight = 1):\n",
    "    params = {}\n",
    "    params[\"objective\"] = \"binary:logistic\"\n",
    "    params[\"eta\"] = 0.01\n",
    "    params[\"min_child_weight\"] = weight\n",
    "    params[\"subsample\"] = 0.8\n",
    "    params[\"colsample_bytree\"] = 0.7\n",
    "    params[\"silent\"] = 1\n",
    "    params[\"max_depth\"] = 10\n",
    "    #params[\"max_delta_step\"]=2\n",
    "    params[\"seed\"] = 0\n",
    "    params['eval_metric'] = \"auc\"\n",
    "    plst = list(params.items())\n",
    "    num_rounds = 2500\n",
    "\n",
    "    xgtrain = xgb.DMatrix(train_X, label=train_y)\n",
    "    xgtest = xgb.DMatrix(test_X)\n",
    "    model = xgb.train(plst, xgtrain, num_rounds)\n",
    "    pred_test_y = model.predict(xgtest)\n",
    "    return pred_test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X = np.array(train_df[features]).astype('float')\n",
    "test_X = np.array(test_df[features]).astype('float')\n",
    "\n",
    "train_y = train_df['is_pass']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print (\"Running model 2..\")\n",
    "pred_test_y = runXGB(train_X, train_y, test_X)\n",
    "\n",
    "sample['is_pass'] = pred_test_y\n",
    "sample.to_csv(\"sub/sub2.csv\", index=False) #0.7937"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print (\"Running model 3..\")\n",
    "pred_test_y = runXGB(train_X, train_y, test_X, weight = 10)\n",
    "\n",
    "sample['is_pass'] = pred_test_y\n",
    "sample.to_csv(\"sub/sub2_1.csv\", index=False) #0.7910"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data...\n"
     ]
    }
   ],
   "source": [
    "print (\"loading data...\")\n",
    "train = pd.read_csv('train_HK6lq50.csv')\n",
    "test = pd.read_csv('test_2nAIblo.csv')\n",
    "\n",
    "sample = pd.read_csv('sample_submission_vaSxamm.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getCountVar(compute_df, count_df, var_name):\n",
    "    grouped_df = count_df.groupby(var_name)\n",
    "    count_dict = {}\n",
    "    for name, group in grouped_df:\n",
    "        count_dict[name] = group.shape[0]\n",
    "\n",
    "    count_list = []\n",
    "    for index, row in compute_df.iterrows():\n",
    "        name = row[var_name]\n",
    "        count_list.append(count_dict.get(name, 0))\n",
    "    return count_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getMeanVar(compute_df, count_df, var_name):\n",
    "    grouped_df = count_df.groupby(var_name)\n",
    "    count_dict = {}\n",
    "    for name, group in grouped_df:\n",
    "        count_dict[name] = np.mean(np.array(group[\"is_pass\"]))\n",
    "\n",
    "    count_list = []\n",
    "    for index, row in compute_df.iterrows():\n",
    "        name = row[var_name]\n",
    "        count_list.append(count_dict.get(name, 0))\n",
    "    return count_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['program_number'] = train.program_id.apply(lambda x: str(x).split('_')[1])\n",
    "test['program_number'] = test.program_id.apply(lambda x: str(x).split('_')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filling missing values\n"
     ]
    }
   ],
   "source": [
    "print (\"filling missing values\")\n",
    "train.age.fillna(np.mean(train.age), inplace=True)\n",
    "test.age.fillna(np.mean(train.age), inplace=True)\n",
    "\n",
    "train['trainee_engagement_rating'].fillna(2, inplace=True)\n",
    "test['trainee_engagement_rating'].fillna(2, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting count features\n"
     ]
    }
   ],
   "source": [
    "print (\"getting count features\")\n",
    "\n",
    "train['program_id_count'] = getCountVar(train,train, \"program_id\")\n",
    "test['program_id_count'] = getCountVar(test,test, \"program_id\")\n",
    "\n",
    "train['program_type_count'] = getCountVar(train,train, \"program_type\")\n",
    "test['program_type_count'] = getCountVar(test,test, \"program_type\")\n",
    "\n",
    "train['test_id_count'] = getCountVar(train,train, \"test_id\")\n",
    "test['test_id_count'] = getCountVar(test,test, \"test_id\")\n",
    "\n",
    "train['trainee_id_count'] = getCountVar(train,train, \"trainee_id\")\n",
    "test['trainee_id_count'] = getCountVar(test,test, \"trainee_id\")\n",
    "\n",
    "train['program_number_count'] = getCountVar(train,train, \"program_number\")\n",
    "test['program_number_count'] = getCountVar(test,test, \"program_number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## print getting mean encoders\n",
    "\n",
    "train['program_id_mean'] = getMeanVar(train,train, \"program_id\")\n",
    "test['program_id_mean'] = getMeanVar(test,train, \"program_id\")\n",
    "\n",
    "train['program_type_mean'] = getMeanVar(train,train, \"program_type\")\n",
    "test['program_type_mean'] = getMeanVar(test,train, \"program_type\")\n",
    "\n",
    "train['test_id_mean'] = getMeanVar(train,train, \"test_id\")\n",
    "test['test_id_mean'] = getMeanVar(test,train, \"test_id\")\n",
    "\n",
    "train['difficulty_level_mean'] = getMeanVar(train,train, \"difficulty_level\")\n",
    "test['difficulty_level_mean'] = getMeanVar(test,train, \"difficulty_level\")\n",
    "\n",
    "train['education_mean'] = getMeanVar(train,train, \"education\")\n",
    "test['education_mean'] = getMeanVar(test,train, \"education\")\n",
    "\n",
    "train['city_tier_mean'] = getMeanVar(train,train, \"city_tier\")\n",
    "test['city_tier_mean'] = getMeanVar(test,train, \"city_tier\")\n",
    "\n",
    "train['program_number_mean'] = getMeanVar(train,train, \"program_number\")\n",
    "test['program_number_mean'] = getMeanVar(test,train, \"program_number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training = train.copy()\n",
    "testing = test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###### one hot encoding\n",
    "feat = ['program_type', 'education', 'program_number']\n",
    "\n",
    "dummies = pd.get_dummies(training[feat], prefix = feat)\n",
    "training = pd.concat([training, dummies], axis=1)\n",
    "\n",
    "dummies = pd.get_dummies(testing[feat], prefix = feat)\n",
    "testing = pd.concat([testing, dummies], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## label encoding\n",
    "\n",
    "feat = ['test_type', 'gender', 'is_handicapped']\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "lb = LabelEncoder()\n",
    "\n",
    "for i in feat:\n",
    "    training[i] = lb.fit_transform(training[i].astype('str'))\n",
    "    testing[i] = lb.transform(testing[i].astype('str'))\n",
    "    \n",
    "    training[i] = training[i].astype('object')\n",
    "    testing[i] = testing[i].astype('object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## for difficulty level\n",
    "#training.difficulty_level.value_counts()\n",
    "dic = {'easy':1, 'intermediate':2, 'hard':3, 'vary hard': 4}\n",
    "\n",
    "training['difficulty_level'] = training['difficulty_level'].map(dic)\n",
    "testing['difficulty_level'] = testing['difficulty_level'].map(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = list(training.drop(['id', 'program_id','program_type', 'trainee_id','education', 'is_pass','test_id'\n",
    "                              ],axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testing.fillna(-1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X = np.array(training[features]).astype('float')\n",
    "test_X = np.array(testing[features]).astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_y = training['is_pass']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shubhamjain/anaconda/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "def runXGB(train_X, train_y, test_X):\n",
    "    params = {}\n",
    "    params[\"objective\"] = \"binary:logistic\"\n",
    "    params[\"eta\"] = 0.01\n",
    "    params[\"min_child_weight\"] = 1\n",
    "    params[\"subsample\"] = 0.8\n",
    "    params[\"colsample_bytree\"] = 0.7\n",
    "    params[\"silent\"] = 1\n",
    "    params[\"max_depth\"] = 10\n",
    "    #params[\"max_delta_step\"]=2\n",
    "    params[\"seed\"] = 0\n",
    "    params['eval_metric'] = \"auc\"\n",
    "    plst = list(params.items())\n",
    "    num_rounds = 2500\n",
    "\n",
    "    xgtrain = xgb.DMatrix(train_X, label=train_y)\n",
    "    xgtest = xgb.DMatrix(test_X)\n",
    "    model = xgb.train(plst, xgtrain, num_rounds)\n",
    "    pred_test_y = model.predict(xgtest)\n",
    "    return pred_test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print (\"Running model..\")\n",
    "pred_test_y = runXGB(train_X, train_y, test_X)\n",
    "\n",
    "sample['is_pass'] = pred_test_y\n",
    "sample.to_csv(\"sub/sub1.csv\", index=False) #0.7499"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 4: Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train_df.csv')\n",
    "test_df = pd.read_csv('test_df.csv')\n",
    "\n",
    "sample = pd.read_csv('sample_submission_vaSxamm.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def runLR(train_X, train_y, test_X, test_y=None, test_X2=None):\n",
    "    model = linear_model.LogisticRegression(fit_intercept=True, C=0.3)\n",
    "    model.fit(train_X, train_y)\n",
    "    print (model.coef_, model.intercept_)\n",
    "    train_preds = model.predict_proba(train_X)[:,1]\n",
    "    test_preds = model.predict_proba(test_X)[:,1]\n",
    "    test_preds2 = model.predict_proba(test_X2)[:,1]\n",
    "    test_loss = 0\n",
    "    if test_y is not None:\n",
    "        train_loss = metrics.roc_auc_score(train_y, train_preds)\n",
    "        test_loss = metrics.roc_auc_score(test_y, test_preds)\n",
    "        print (\"Train and Test loss : \", train_loss, test_loss)\n",
    "    return test_preds, test_loss, test_preds2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def runET(train_X, train_y, test_X, test_y=None, test_X2=None, depth=10, leaf=5, feat=0.3):\n",
    "    model = ensemble.ExtraTreesClassifier(\n",
    "            n_estimators = 300,\n",
    "                    max_depth = depth,\n",
    "                    min_samples_split = 10,\n",
    "                    min_samples_leaf = leaf,\n",
    "                    max_features =  feat,\n",
    "                    n_jobs = 6,\n",
    "                    random_state = 0)\n",
    "    model.fit(train_X, train_y)\n",
    "    train_preds = model.predict_proba(train_X)[:,1]\n",
    "    test_preds = model.predict_proba(test_X)[:,1]\n",
    "    test_preds2 = model.predict_proba(test_X2)[:,1]\n",
    "    test_loss = 0\n",
    "    if test_y is not None:\n",
    "        train_loss = metrics.roc_auc_score(train_y, train_preds)\n",
    "        test_loss = metrics.roc_auc_score(test_y, test_preds)\n",
    "        print (\"Depth, leaf, feat : \", depth, leaf, feat)\n",
    "        print (\"Train and Test loss : \", train_loss, test_loss)\n",
    "    return test_preds, test_loss, test_preds2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "def runLGB(train_X, train_y, test_X, test_y=None, test_X2=None, feature_names=None, seed_val=0, rounds=500, dep=3, eta=0.001):\n",
    "    params = {}\n",
    "    params[\"objective\"] = \"binary\"\n",
    "    params['metric'] = 'auc'\n",
    "    params[\"max_depth\"] = dep\n",
    "    params[\"min_data_in_leaf\"] = 100\n",
    "    params[\"learning_rate\"] = eta\n",
    "    params[\"bagging_fraction\"] = 0.7\n",
    "    params[\"feature_fraction\"] = 0.7\n",
    "    params[\"bagging_freq\"] = 5\n",
    "    params[\"bagging_seed\"] = seed_val\n",
    "    params[\"verbosity\"] = -1\n",
    "    num_rounds = rounds\n",
    "\n",
    "    plst = list(params.items())\n",
    "    lgtrain = lgb.Dataset(train_X, label=train_y)\n",
    "\n",
    "    if test_y is not None:\n",
    "        lgtest = lgb.Dataset(test_X, label=test_y)\n",
    "        model = lgb.train(params, lgtrain, num_rounds, valid_sets=[lgtest], early_stopping_rounds=100, verbose_eval=20)\n",
    "    else:\n",
    "        lgtest = lgb.DMatrix(test_X)\n",
    "        model = lgb.train(params, lgtrain, num_rounds)\n",
    "\n",
    "    pred_test_y = model.predict(test_X, num_iteration=model.best_iteration)\n",
    "    pred_test_y2 = model.predict(test_X2, num_iteration=model.best_iteration)\n",
    "\n",
    "    loss = 0\n",
    "    if test_y is not None:\n",
    "        loss = metrics.roc_auc_score(test_y, pred_test_y)\n",
    "        print (loss)\n",
    "        return pred_test_y, loss, pred_test_y2\n",
    "    else:\n",
    "        return pred_test_y, loss, pred_test_y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def runXGB(train_X, train_y, test_X, test_y=None, test_X2=None, feature_names=None, seed_val=0, rounds=500, dep=8, eta=0.001):\n",
    "        params = {}\n",
    "        params[\"objective\"] = \"binary:logistic\"\n",
    "        params['eval_metric'] = 'auc'\n",
    "        params[\"eta\"] = eta\n",
    "        params[\"subsample\"] = 0.7\n",
    "        params[\"min_child_weight\"] = 10\n",
    "        params[\"colsample_bytree\"] = 0.7\n",
    "        params[\"max_depth\"] = dep\n",
    "        params[\"silent\"] = 1\n",
    "        params[\"seed\"] = seed_val\n",
    "        #params[\"max_delta_step\"] = 2\n",
    "        #params[\"gamma\"] = 0.5\n",
    "        num_rounds = rounds\n",
    "\n",
    "        plst = list(params.items())\n",
    "        xgtrain = xgb.DMatrix(train_X, label=train_y)\n",
    "\n",
    "        if test_y is not None:\n",
    "                xgtest = xgb.DMatrix(test_X, label=test_y)\n",
    "                watchlist = [ (xgtrain,'train'), (xgtest, 'test') ]\n",
    "                model = xgb.train(plst, xgtrain, num_rounds, watchlist, early_stopping_rounds=100, verbose_eval=20)\n",
    "        else:\n",
    "                xgtest = xgb.DMatrix(test_X)\n",
    "                model = xgb.train(plst, xgtrain, num_rounds)\n",
    "\n",
    "        pred_test_y = model.predict(xgtest, ntree_limit=model.best_ntree_limit)\n",
    "        pred_test_y2 = model.predict(xgb.DMatrix(test_X2), ntree_limit=model.best_ntree_limit)\n",
    "\n",
    "        loss = 0\n",
    "        if test_y is not None:\n",
    "                loss = metrics.log_loss(test_y, pred_test_y)\n",
    "                print (loss)\n",
    "                return pred_test_y, loss, pred_test_y2\n",
    "        else:\n",
    "                return pred_test_y, loss, pred_test_y2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols_to_use = list(train_df.drop(['id', 'program_id','program_type', 'trainee_id','education', 'is_pass'\n",
    "                              ],axis=1))\n",
    "\n",
    "train_X = train_df[cols_to_use]\n",
    "test_X = test_df[cols_to_use]\n",
    "\n",
    "train_y = train_df['is_pass']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_unique_programs = np.array(train_df[\"program_type\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=98765)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print (\"Model building..\")\n",
    "model_name = \"XGB\"\n",
    "cv_scores = []\n",
    "pred_test_full = 0\n",
    "pred_val_full = np.zeros(train_df.shape[0])\t\n",
    "for dev_index, val_index in kf.split(train_unique_programs):\n",
    "#for [dev_camp, val_camp] in camp_indices:\n",
    "    dev_camp, val_camp = train_unique_programs[dev_index].tolist(), train_unique_programs[val_index].tolist()\n",
    "    dev_X, val_X = train_X[train_df['program_type'].isin(dev_camp)], train_X[train_df['program_type'].isin(val_camp)]\n",
    "    dev_y, val_y = train_y[train_df['program_type'].isin(dev_camp)], train_y[train_df['program_type'].isin(val_camp)]\n",
    "    print (dev_X.shape, val_X.shape)\n",
    "\n",
    "    if model_name == \"LGB\":\n",
    "        pred_val1, loss1, pred_test1 = runLGB(dev_X, dev_y, val_X, val_y, test_X, rounds=5000, dep=4)\n",
    "        pred_val2, loss2, pred_test2 = runLGB(dev_X, dev_y, val_X, val_y, test_X, rounds=5000, dep=4, seed_val=2018)\n",
    "        pred_val3, loss3, pred_test3 = runLGB(dev_X, dev_y, val_X, val_y, test_X, rounds=5000, dep=4, seed_val=9876)\n",
    "        pred_val = (pred_val1 + pred_val2 + pred_val3)/3. \n",
    "        pred_test = (pred_test1 + pred_test2 + pred_test3)/3.\n",
    "        loss = (loss1 + loss2 + loss3)/3. \n",
    "    elif model_name == \"XGB\":\n",
    "        pred_val1, loss1, pred_test1 = runXGB(dev_X, dev_y, val_X, val_y, test_X, rounds=5000, dep=4)\n",
    "        pred_val2, loss2, pred_test2 = runXGB(dev_X, dev_y, val_X, val_y, test_X, rounds=5000, dep=4, seed_val=2018)\n",
    "        pred_val3, loss3, pred_test3 = runXGB(dev_X, dev_y, val_X, val_y, test_X, rounds=5000, dep=4, seed_val=9876)\n",
    "        pred_val = (pred_val1 + pred_val2 + pred_val3)/3. \n",
    "        pred_test = (pred_test1 + pred_test2 + pred_test3)/3.\n",
    "        loss = (loss1 + loss2 + loss3)/3. \n",
    "    elif model_name == \"ET\":\n",
    "        pred_val, loss, pred_test = runET(dev_X, dev_y, val_X, val_y, test_X, depth=20, leaf=20, feat=0.3)\n",
    "    elif model_name == \"LR\":\n",
    "        pred_val, loss, pred_test = runLR(dev_X, dev_y, val_X, val_y, test_X)\n",
    "\n",
    "    pred_test_full += pred_test\n",
    "    pred_val_full[train_df['program_type'].isin(val_camp)] = pred_val\n",
    "    loss = metrics.roc_auc_score(train_y[train_df['program_type'].isin(val_camp)], pred_val)\n",
    "    cv_scores.append(loss)\n",
    "    print (cv_scores)\n",
    "\n",
    "\n",
    "pred_test_full /= 5.\n",
    "print (np.mean(cv_scores), metrics.roc_auc_score(train_y, pred_val_full))\n",
    "\n",
    "#sub_df = pd.DataFrame({\"id\":test_id})\n",
    "sample[\"is_pass\"] = pred_test_full\n",
    "sample.to_csv(\"sub/stack_xgb.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print (\"Model building..\")\n",
    "model_name = \"LGB\"\n",
    "cv_scores = []\n",
    "pred_test_full = 0\n",
    "pred_val_full = np.zeros(train_df.shape[0])\t\n",
    "for dev_index, val_index in kf.split(train_unique_programs):\n",
    "#for [dev_camp, val_camp] in camp_indices:\n",
    "    dev_camp, val_camp = train_unique_programs[dev_index].tolist(), train_unique_programs[val_index].tolist()\n",
    "    dev_X, val_X = train_X[train_df['program_type'].isin(dev_camp)], train_X[train_df['program_type'].isin(val_camp)]\n",
    "    dev_y, val_y = train_y[train_df['program_type'].isin(dev_camp)], train_y[train_df['program_type'].isin(val_camp)]\n",
    "    print (dev_X.shape, val_X.shape)\n",
    "\n",
    "    if model_name == \"LGB\":\n",
    "        pred_val1, loss1, pred_test1 = runLGB(dev_X, dev_y, val_X, val_y, test_X, rounds=5000, dep=4)\n",
    "        pred_val2, loss2, pred_test2 = runLGB(dev_X, dev_y, val_X, val_y, test_X, rounds=5000, dep=4, seed_val=2018)\n",
    "        pred_val3, loss3, pred_test3 = runLGB(dev_X, dev_y, val_X, val_y, test_X, rounds=5000, dep=4, seed_val=9876)\n",
    "        pred_val = (pred_val1 + pred_val2 + pred_val3)/3. \n",
    "        pred_test = (pred_test1 + pred_test2 + pred_test3)/3.\n",
    "        loss = (loss1 + loss2 + loss3)/3. \n",
    "    elif model_name == \"XGB\":\n",
    "        pred_val1, loss1, pred_test1 = runXGB(dev_X, dev_y, val_X, val_y, test_X, rounds=5000, dep=4)\n",
    "        pred_val2, loss2, pred_test2 = runXGB(dev_X, dev_y, val_X, val_y, test_X, rounds=5000, dep=4, seed_val=2018)\n",
    "        pred_val3, loss3, pred_test3 = runXGB(dev_X, dev_y, val_X, val_y, test_X, rounds=5000, dep=4, seed_val=9876)\n",
    "        pred_val = (pred_val1 + pred_val2 + pred_val3)/3. \n",
    "        pred_test = (pred_test1 + pred_test2 + pred_test3)/3.\n",
    "        loss = (loss1 + loss2 + loss3)/3. \n",
    "    elif model_name == \"ET\":\n",
    "        pred_val, loss, pred_test = runET(dev_X, dev_y, val_X, val_y, test_X, depth=20, leaf=20, feat=0.3)\n",
    "    elif model_name == \"LR\":\n",
    "        pred_val, loss, pred_test = runLR(dev_X, dev_y, val_X, val_y, test_X)\n",
    "\n",
    "    pred_test_full += pred_test\n",
    "    pred_val_full[train_df['program_type'].isin(val_camp)] = pred_val\n",
    "    loss = metrics.roc_auc_score(train_y[train_df['program_type'].isin(val_camp)], pred_val)\n",
    "    cv_scores.append(loss)\n",
    "    print (cv_scores)\n",
    "\n",
    "\n",
    "pred_test_full /= 5.\n",
    "print (np.mean(cv_scores), metrics.roc_auc_score(train_y, pred_val_full))\n",
    "\n",
    "#sub_df = pd.DataFrame({\"id\":test_id})\n",
    "sample[\"is_pass\"] = pred_test_full\n",
    "sample.to_csv(\"sub/stack_lgb.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "print (\"Model building..\")\n",
    "model_name = \"LR\"\n",
    "cv_scores = []\n",
    "pred_test_full = 0\n",
    "pred_val_full = np.zeros(train_df.shape[0])\t\n",
    "for dev_index, val_index in kf.split(train_unique_programs):\n",
    "#for [dev_camp, val_camp] in camp_indices:\n",
    "    dev_camp, val_camp = train_unique_programs[dev_index].tolist(), train_unique_programs[val_index].tolist()\n",
    "    dev_X, val_X = train_X[train_df['program_type'].isin(dev_camp)], train_X[train_df['program_type'].isin(val_camp)]\n",
    "    dev_y, val_y = train_y[train_df['program_type'].isin(dev_camp)], train_y[train_df['program_type'].isin(val_camp)]\n",
    "    print (dev_X.shape, val_X.shape)\n",
    "\n",
    "    if model_name == \"LGB\":\n",
    "        pred_val1, loss1, pred_test1 = runLGB(dev_X, dev_y, val_X, val_y, test_X, rounds=5000, dep=4)\n",
    "        pred_val2, loss2, pred_test2 = runLGB(dev_X, dev_y, val_X, val_y, test_X, rounds=5000, dep=4, seed_val=2018)\n",
    "        pred_val3, loss3, pred_test3 = runLGB(dev_X, dev_y, val_X, val_y, test_X, rounds=5000, dep=4, seed_val=9876)\n",
    "        pred_val = (pred_val1 + pred_val2 + pred_val3)/3. \n",
    "        pred_test = (pred_test1 + pred_test2 + pred_test3)/3.\n",
    "        loss = (loss1 + loss2 + loss3)/3. \n",
    "    elif model_name == \"XGB\":\n",
    "        pred_val1, loss1, pred_test1 = runXGB(dev_X, dev_y, val_X, val_y, test_X, rounds=5000, dep=4)\n",
    "        pred_val2, loss2, pred_test2 = runXGB(dev_X, dev_y, val_X, val_y, test_X, rounds=5000, dep=4, seed_val=2018)\n",
    "        pred_val3, loss3, pred_test3 = runXGB(dev_X, dev_y, val_X, val_y, test_X, rounds=5000, dep=4, seed_val=9876)\n",
    "        pred_val = (pred_val1 + pred_val2 + pred_val3)/3. \n",
    "        pred_test = (pred_test1 + pred_test2 + pred_test3)/3.\n",
    "        loss = (loss1 + loss2 + loss3)/3. \n",
    "    elif model_name == \"ET\":\n",
    "        pred_val, loss, pred_test = runET(dev_X, dev_y, val_X, val_y, test_X, depth=20, leaf=20, feat=0.3)\n",
    "    elif model_name == \"LR\":\n",
    "        pred_val, loss, pred_test = runLR(dev_X, dev_y, val_X, val_y, test_X)\n",
    "\n",
    "    pred_test_full += pred_test\n",
    "    pred_val_full[train_df['program_type'].isin(val_camp)] = pred_val\n",
    "    loss = metrics.roc_auc_score(train_y[train_df['program_type'].isin(val_camp)], pred_val)\n",
    "    cv_scores.append(loss)\n",
    "    print (cv_scores)\n",
    "\n",
    "\n",
    "pred_test_full /= 5.\n",
    "print (np.mean(cv_scores), metrics.roc_auc_score(train_y, pred_val_full))\n",
    "\n",
    "#sub_df = pd.DataFrame({\"id\":test_id})\n",
    "sample[\"is_pass\"] = pred_test_full\n",
    "sample.to_csv(\"sub/stack_lr.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import ensemble\n",
    "print (\"Model building..\")\n",
    "model_name = \"ET\"\n",
    "cv_scores = []\n",
    "pred_test_full = 0\n",
    "pred_val_full = np.zeros(train_df.shape[0])\t\n",
    "for dev_index, val_index in kf.split(train_unique_programs):\n",
    "#for [dev_camp, val_camp] in camp_indices:\n",
    "    dev_camp, val_camp = train_unique_programs[dev_index].tolist(), train_unique_programs[val_index].tolist()\n",
    "    dev_X, val_X = train_X[train_df['program_type'].isin(dev_camp)], train_X[train_df['program_type'].isin(val_camp)]\n",
    "    dev_y, val_y = train_y[train_df['program_type'].isin(dev_camp)], train_y[train_df['program_type'].isin(val_camp)]\n",
    "    print (dev_X.shape, val_X.shape)\n",
    "\n",
    "    if model_name == \"LGB\":\n",
    "        pred_val1, loss1, pred_test1 = runLGB(dev_X, dev_y, val_X, val_y, test_X, rounds=5000, dep=4)\n",
    "        pred_val2, loss2, pred_test2 = runLGB(dev_X, dev_y, val_X, val_y, test_X, rounds=5000, dep=4, seed_val=2018)\n",
    "        pred_val3, loss3, pred_test3 = runLGB(dev_X, dev_y, val_X, val_y, test_X, rounds=5000, dep=4, seed_val=9876)\n",
    "        pred_val = (pred_val1 + pred_val2 + pred_val3)/3. \n",
    "        pred_test = (pred_test1 + pred_test2 + pred_test3)/3.\n",
    "        loss = (loss1 + loss2 + loss3)/3. \n",
    "    elif model_name == \"XGB\":\n",
    "        pred_val1, loss1, pred_test1 = runXGB(dev_X, dev_y, val_X, val_y, test_X, rounds=5000, dep=4)\n",
    "        pred_val2, loss2, pred_test2 = runXGB(dev_X, dev_y, val_X, val_y, test_X, rounds=5000, dep=4, seed_val=2018)\n",
    "        pred_val3, loss3, pred_test3 = runXGB(dev_X, dev_y, val_X, val_y, test_X, rounds=5000, dep=4, seed_val=9876)\n",
    "        pred_val = (pred_val1 + pred_val2 + pred_val3)/3. \n",
    "        pred_test = (pred_test1 + pred_test2 + pred_test3)/3.\n",
    "        loss = (loss1 + loss2 + loss3)/3. \n",
    "    elif model_name == \"ET\":\n",
    "        pred_val, loss, pred_test = runET(dev_X, dev_y, val_X, val_y, test_X, depth=20, leaf=20, feat=0.3)\n",
    "    elif model_name == \"LR\":\n",
    "        pred_val, loss, pred_test = runLR(dev_X, dev_y, val_X, val_y, test_X)\n",
    "\n",
    "    pred_test_full += pred_test\n",
    "    pred_val_full[train_df['program_type'].isin(val_camp)] = pred_val\n",
    "    loss = metrics.roc_auc_score(train_y[train_df['program_type'].isin(val_camp)], pred_val)\n",
    "    cv_scores.append(loss)\n",
    "    print (cv_scores)\n",
    "\n",
    "\n",
    "pred_test_full /= 5.\n",
    "print (np.mean(cv_scores), metrics.roc_auc_score(train_y, pred_val_full))\n",
    "\n",
    "#sub_df = pd.DataFrame({\"id\":test_id})\n",
    "sample[\"is_pass\"] = pred_test_full\n",
    "sample.to_csv(\"sub/stack_et.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub2 = pd.read_csv('sub/sub2.csv')\n",
    "sub_stack = pd.read_csv('sub/stack_xgb.csv')\n",
    "sub1 = pd.read_csv('sub/sub1.csv')\n",
    "sub_lr = pd.read_csv('sub/stack_lr.csv')\n",
    "sub_et = pd.read_csv('sub/stack_et.csv')\n",
    "sub_lgb = pd.read_csv('sub/stack_lgb.csv')\n",
    "sub2_1 = pd.read_csv('sub/sub2_1.csv')\n",
    "sub_xgb = pd.read_csv('sub/xgb.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample['is_pass'] = (sub2['is_pass']  + sub_stack['is_pass']  + sub1['is_pass'] + \n",
    "                     sub_lgb['is_pass'] \n",
    "                     + sub_lr['is_pass']\n",
    "                    + sub_et['is_pass']  + sub2_1['is_pass']  + sub_xgb['is_pass'])/8\n",
    "sample['is_pass'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample.to_csv(\"sub/avg._stacking.csv\", index=False) ## 0.8161"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
